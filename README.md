A simple two-layer neural network and trained it on the MNIST digit recognizer dataset. It's meant to be an instructional to understand the underlying math.


The NN will have a simple two-layer architecture. Input layer  ğ‘[0]  will have 784 units corresponding to the 784 pixels in each 28x28 input image. A hidden layer ğ‘[1]  will have 10 units with ReLU activation, and finally our output layer  ğ‘[2]  will have 10 units corresponding to the ten digit classes with softmax activation.

**Forward propagation**

ğ‘[1]=ğ‘Š[1]ğ‘‹+ğ‘[1]
ğ´[1]=ğ‘”ReLU(ğ‘[1]))
ğ‘[2]=ğ‘Š[2]ğ´[1]+ğ‘[2]
ğ´[2]=ğ‘”softmax(ğ‘[2])

**Backward propagation**

ğ‘‘ğ‘[2]=ğ´[2]âˆ’ğ‘Œ
ğ‘‘ğ‘Š[2]=1ğ‘šğ‘‘ğ‘[2]ğ´[1]ğ‘‡
ğ‘‘ğµ[2]=1ğ‘šÎ£ğ‘‘ğ‘[2]
ğ‘‘ğ‘[1]=ğ‘Š[2]ğ‘‡ğ‘‘ğ‘[2].âˆ—ğ‘”[1]â€²(ğ‘§[1])
ğ‘‘ğ‘Š[1]=1ğ‘šğ‘‘ğ‘[1]ğ´[0]ğ‘‡
ğ‘‘ğµ[1]=1ğ‘šÎ£ğ‘‘ğ‘[1]

**Parameter updates**

ğ‘Š[2]:=ğ‘Š[2]âˆ’ğ›¼ğ‘‘ğ‘Š[2]
ğ‘[2]:=ğ‘[2]âˆ’ğ›¼ğ‘‘ğ‘[2]
ğ‘Š[1]:=ğ‘Š[1]âˆ’ğ›¼ğ‘‘ğ‘Š[1]
ğ‘[1]:=ğ‘[1]âˆ’ğ›¼ğ‘‘ğ‘[1]

**Vars and shapes**

Forward prop

ğ´[0]=ğ‘‹ : 784 x m
ğ‘[1]âˆ¼ğ´[1] : 10 x m
ğ‘Š[1] : 10 x 784 (as  ğ‘Š[1]ğ´[0]âˆ¼ğ‘[1] )
ğµ[1] : 10 x 1
ğ‘[2]âˆ¼ğ´[2] : 10 x m
ğ‘Š[1] : 10 x 10 (as  ğ‘Š[2]ğ´[1]âˆ¼ğ‘[2] )
ğµ[2] : 10 x 1

Backprop

ğ‘‘ğ‘[2] : 10 x m (  ğ´[2] )
ğ‘‘ğ‘Š[2] : 10 x 10
ğ‘‘ğµ[2] : 10 x 1
ğ‘‘ğ‘[1] : 10 x m (  ğ´[1] )
ğ‘‘ğ‘Š[1] : 10 x 10
ğ‘‘ğµ[1] : 10 x 1
